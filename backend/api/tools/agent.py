
from smolagents import CodeAgent, LiteLLMModel

from .context_building import build_context
from smolagents import LiteLLMModel

DEBUG = True

def define_models() -> dict:
  return None

class Agent:
  fixed_questions: list[dict] = None
  models: dict = None

  def __init__(self, fixed_questions=None):
    self.fixed_questions = fixed_questions if fixed_questions is not None else []
    self.models = define_models()
  
  def update_fixed_questions(self, fixed_questions):
    """Update the fixed questions for the agent"""
    self.fixed_questions = fixed_questions
    print(f"Agent updated with {len(fixed_questions)} fixed questions")

  def forward(self, body: list[dict]) -> list[dict]:
    """
    Processes the input body and returns a response based on the agent's logic.

    Args:
        body (list[dict]): The input body containing user messages and other relevant information.

    Returns:
        list[dict]: The response generated by the agent.
    """
    # Here you would implement the logic to process the body and generate a response
    translated_body = translate_json(body)
    anthropified_body = anthropify_body(translated_body)
    
    # Add fixed questions to the beginning of the context if they exist
    if self.fixed_questions:
      # Add fixed questions as initial context
      fixed_context = []
      for fq in self.fixed_questions:
        # Add the question as assistant message
        fixed_context.append({"role": "assistant", "content": fq["question"]})
        # Add the answer as user message
        fixed_context.append({"role": "user", "content": fq["answer"]})
      
      # Prepend fixed questions to the conversation
      anthropified_body = fixed_context + anthropified_body
      
      if DEBUG:
        print("Added fixed questions to context:", len(self.fixed_questions))
    
    if DEBUG:
      print("Anthropified body:", anthropified_body)
    context = build_context(anthropified_body)
    if DEBUG:
      print("Context built:", context)
    
    # Context is user/assistant alternating messages
  
    # Define answer by LLMlite
    with open("./api_key.txt", "r") as f:
        key = f.read().strip()
    model = LiteLLMModel(model_id="claude-3-5-haiku-latest", temperature=0.3, max_tokens=300, api_key=key)

    answer = model.generate(context)

    # Returns a {"text":..., "balise":...}
    return {"balise": "cours", "text": answer.content}

agent = Agent()

def init_agent(fixed_questions: list[dict]):
  """Initialize the agent with fixed questions from the questionnaire"""
  agent.update_fixed_questions(fixed_questions)
  print(f"Agent initialized with {len(fixed_questions)} fixed questions")

def run_agent(body: list[dict]) -> list[dict]:
   return agent.forward(body)

def define_models() -> dict:
  models = dict()
  with open("./api_key.txt", "r") as f:
    api_key = f.read().strip()
  
  models["course"] = CodeAgent(model=LiteLLMModel(model_id="claude-3-5-haiku-latest", api_key=api_key, temperature=0.2, max_tokens=2000), name="course_agent", description="This Agent is responsible for explaining large concepts to the user. If the user seems stuck on some specific content, the course_agent would be interesting to use to explain the user the concept he is lacking.  It is also used to answer the user's questions about the course, such as 'What is a Markov Chain?' or 'What is Young's double-slit experiment?'. The course_agent is also used to explain the course step by step, and to answer the user's questions about the course.")

  models["question"] = CodeAgent(model=LiteLLMModel(model_id="claude-3-5-haiku-latest", api_key=api_key, temperature=0.2, max_tokens=1000), name="question_agent", description="This Agent is responsible for asking questions to the user to verify the users understanding of the course. When the user seems to have understood the course, the question_agent will ask questions to verify the user's understanding. If the user seems to be stuck on some specific content, the question_agent will ask questions to help the user understand the concept he is lacking. The question_agent is also used to ask questions about the user's understanding of the course, such as 'Do you feel like you completely understand what a Markov Chain is?' or 'Do you know the principle behind Young's double-slit experiment?'.")

  models["manager"] = CodeAgent(model=LiteLLMModel(model_id="claude-3-5-haiku-latest", api_key=api_key, temperature=0.1, max_tokens=1000), managed_agents=[models["course"], models["question"]], name="manager_agent", description="This Agent is responsible for managing the conversation between the user and the course_agent and question_agent. It decides which agent to use based on the user's input and the context of the conversation. The manager_agent can also choose not to do anything if it believes it wouldn't help the user, or the user did not write enough to warrant an answer.")

  return models

def question_agent (context : str) -> str:
  """
  determines if enough inital questions have been asked to start the conversation, if not, it will ask the user a question
  """
  api_key = open(r"backend\api\tools\storage\api_key.txt", "r", encoding="utf-8").read()
  model = LiteLLMModel(
    model_id="anthropic/claude-opus-4-20250514",
    temperature=1,
    api_key= api_key# in practice we would not hardcode the API key, but use an environment variable or a secure vault service
  )
  
  prompt = """
Role: You're a tutor AI determining if enough context has been gathered to begin teaching.  
Rules:  
1. Analyze ONLY the conversation history below  
2. If essential teaching context is present (subject, level, etc..):  
   - Output EXACTLY: `%Question_end%`  
   - Do not be strict, but ensure the context is sufficient to start teaching
3. If a CRITICAL and ESSENTIAL information is missing:  
   - Output ONE question to gather the MOST URGENT missing information  
   - Phrase it as a SINGLE question only (no prefixes/suffixes)  

Conversation History CONSIDER THE FOLLOWING ONLY, IMPORTANT:  
{history}  

Output Instructions:  
- ONLY `%Question_end%` or ONE question  
- NO explanations, NO markdown, NO apologies  
- Example valid outputs:  
    -`%Question_end%`  
    -`What specific topic in algebra do you need help with?`  
    -`Are you preparing for an exam or just general practice?`  
"""
  prompt = str(prompt.format(history = context))
  return model(
     messages = [{"role": "user", "content": prompt}],
      max_tokens=1000
  ).content


def translate_json(body: list[dict]) -> list[dict]:
    # Gets dict, ordonned by keys and put into list
    translated = []
    for k in sorted(body.keys()):
       translated.append({"balise": body[k]["balise"], "content": body[k]["content"]})
    return translated

def anthropify_body(body: list[dict]) -> list[dict]:
    """
    Converts the body to a format suitable for anthropic models.
    """
    new_body = []
    if body[0]["balise"] == "assistant":
      new_body = [{"role": "user", "content": ""}]
    for message in body:
        if message["balise"] == "user":
            if len(new_body)==0 or new_body[-1]["role"] == "assistant":
               new_body.append({"role": "user", "content": message["content"]})
            else:
               new_body[-1]["content"] += "\n" + message["content"]
        else:
            # If message is not from the user, include balise
            assistant_content = "### " + message["balise"] + " ###\n" + message["content"]
            if len(new_body)==0 or new_body[-1]["role"] == "user":
                new_body.append({"role": "assistant", "content": assistant_content})
            else:
                new_body[-1]["content"] += "\n" + assistant_content

    return new_body